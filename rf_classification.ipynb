{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Image Classification\n",
    "\n",
    "This code steps through an entire random forest classification. 99.9% of the code is provided, you just need to make sure that you **enter the proper names of your orthomosaic and training data sets**.\n",
    "\n",
    "This is important: Python doesn't like long file paths. We need to try to not bury this folder many subfolders deep. Maybe place the folder on the Desktop even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imagery\n",
    "\n",
    "This chunk simply reads in the UAV orthomosaic we generated in Metashape. Please make sure to load your georeferenced orthomosaic with the projected CRS. Make sure to copy it to the same folder as the notebook file. After reading in the image, the code plots the normal orthomosaic. In the next step, we want to make sure we only have three bands. If there are more, we need to remove the 4th band which is just a transparency layer that we don't need.\n",
    "\n",
    "Lastly, we plot all three bands separately. You can switch back and forth between the regular orthomosaic, the three bands, and some information on the bands by clicking on the rectangles below the code chunk.\n",
    "\n",
    "**REPLACE THE NAME OF THE ORTHOMOSAIC WITH YOUR OWN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raster with rioxarray\n",
    "# --> change the name of the tif file to whatever your orthomosaic is called!\n",
    "img_path = \"Ortho3GCPsEPSG6342.tif\"  # 3-band RGB UAV orthomosaic\n",
    "img = rxr.open_rasterio(img_path)\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Number of bands: {len(img.band)}\")\n",
    "print(f\"CRS: {img.rio.crs}\")\n",
    "\n",
    "# If there are more than 3 bands, keep only the first three (R, G, B)\n",
    "if len(img.band) > 3:\n",
    "    print(\"More than 3 bands detected. Keeping only RGB bands.\")\n",
    "    img = img.isel(band=slice(0, 3))\n",
    "\n",
    "# Assign band names for clarity\n",
    "img = img.assign_coords(band=['R', 'G', 'B'])\n",
    "print(f\"\\nBand names: {list(img.band.values)}\")\n",
    "\n",
    "# Plot RGB image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot RGB composite (stretch to 2nd-98th percentile for better visualization)\n",
    "rgb_stretched = img.clip(img.quantile(0.02), img.quantile(0.98))\n",
    "rgb_normalized = (rgb_stretched - rgb_stretched.min()) / (rgb_stretched.max() - rgb_stretched.min())\n",
    "rgb_plot = np.transpose(rgb_normalized.values, (1, 2, 0))\n",
    "axes[0].imshow(rgb_plot)\n",
    "axes[0].set_title('RGB Composite')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot individual bands\n",
    "img.plot.imshow(col='band', col_wrap=3, figsize=(12, 4), \n",
    "                cmap='gray', add_colorbar=True)\n",
    "plt.suptitle('Individual Bands')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Training Data\n",
    "\n",
    "Here we just read the training data geopackage into Python that we created in QGIS. Change the file name of the training data set to what yours is called. **REPLACE THE NAME OF THE TRAINING GPKG WITH YOUR OWN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training polygons\n",
    "# --> enter the name of your training data .gpkg\n",
    "trn = gpd.read_file(\"classes.gpkg\")\n",
    "\n",
    "# Make sure the class column is a categorical type\n",
    "trn['class'] = trn['class'].astype('category')\n",
    "\n",
    "# Quick look\n",
    "print(\"Training data info:\")\n",
    "print(trn.info())\n",
    "print(\"\\nClass counts:\")\n",
    "print(trn['class'].value_counts())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(trn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add an ignore zone\n",
    "\n",
    "THIS IS PROBABLY NOT NECESSARY. SKIP UNTIL YOU THINK YOU MIGHT NEED IT. This chunk adds a layer that will be masked out in the imagery because it contains unnaturally bright colors or other areas which might confuse the indices.\n",
    "\n",
    "THIS IS NOT NECESSARY IN MOST CASES.\n",
    "\n",
    "Only go back to this chunk if something seems off in the imagery or calculated indices. Since we probably won't need it, this code won't get executed when the entire script is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ignore Zone Masking ---\n",
    "# Uncomment and run this cell only if you need to mask out specific areas\n",
    "\n",
    "# # Read polygons marking areas to ignore\n",
    "# ignore = gpd.read_file(\"ignore_zones_epsg_6342.gpkg\")\n",
    "\n",
    "# # Create a mask from the ignore polygons\n",
    "# from rasterio import features\n",
    "# from rasterio.transform import from_bounds\n",
    "\n",
    "# # Create a mask raster from the ignore polygons\n",
    "# mask_array = features.rasterize(\n",
    "#     ignore.geometry,\n",
    "#     out_shape=img.shape[1:],\n",
    "#     transform=img.rio.transform(),\n",
    "#     fill=1,  # areas to keep\n",
    "#     default_value=0  # areas to ignore\n",
    "# )\n",
    "\n",
    "# # Apply mask to the image (inverse=True means \"keep everything outside polygons\")\n",
    "# img_masked = img.where(mask_array == 1)\n",
    "# print(\"Image masked successfully\")\n",
    "\n",
    "# # Quick check: plot with masked areas\n",
    "# rgb_masked_plot = np.transpose(img_masked.values, (1, 2, 0))\n",
    "# rgb_masked_plot = (rgb_masked_plot - np.nanmin(rgb_masked_plot)) / (np.nanmax(rgb_masked_plot) - np.nanmin(rgb_masked_plot))\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(rgb_masked_plot)\n",
    "# plt.title('RGB with Ignore Zone Masked')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple RGB Indices\n",
    "\n",
    "We only have three bands, so many of the cool indices available for multispectral data are impossible for us to generate. However, there are some indices that work with just RGB data. We will derive some of them and then also check whether they add much information or not. This chunk creates the indices and then stacks them all together in one variable called stk.\n",
    "\n",
    "Pay attention to the plot at the end of the code chunk. You want to make sure that there is variability within each of the bands and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to 0-1\n",
    "# Check if masked image exists, otherwise use the normal image\n",
    "try:\n",
    "    img01 = img_masked / 255.0\n",
    "    print(\"Using masked image\")\n",
    "except NameError:\n",
    "    img01 = img / 255.0\n",
    "    print(\"Using original image\")\n",
    "\n",
    "# Use a tiny epsilon to prevent division by zero\n",
    "eps = 1e-10\n",
    "\n",
    "# Extract individual bands\n",
    "R = img01.sel(band='R')\n",
    "G = img01.sel(band='G')\n",
    "B = img01.sel(band='B')\n",
    "\n",
    "# Calculate indices\n",
    "# Visible Atmospherically Resistant Index (VARI)\n",
    "VARI = (G - R) / (G + R - B + eps)\n",
    "# Excess Green (ExG)\n",
    "ExG = 2 * G - R - B\n",
    "# Normalized Difference Index (NDI, Blueâ€“Red)\n",
    "NDI = (B - R) / (B + R + eps)\n",
    "# Green-Red Vegetation Index (GRVI)\n",
    "GRVI = (G - R) / (G + R + B + eps)\n",
    "\n",
    "# Clamp extreme outliers so plots look reasonable\n",
    "VARI = VARI.clip(-1, 1)\n",
    "ExG = ExG.clip(-1, 1)\n",
    "NDI = NDI.clip(-1, 1)\n",
    "GRVI = GRVI.clip(-1, 1)\n",
    "\n",
    "# Stack all bands and indices together\n",
    "stk = xr.concat([R, G, B, VARI, ExG, NDI, GRVI], \n",
    "                dim='band')\n",
    "stk = stk.assign_coords(band=['R', 'G', 'B', 'VARI', 'ExG', 'NDI', 'GRVI'])\n",
    "\n",
    "print(f\"Stacked array shape: {stk.shape}\")\n",
    "print(f\"Band names: {list(stk.band.values)}\")\n",
    "\n",
    "# Plot all bands and indices\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, band_name in enumerate(stk.band.values):\n",
    "    if i < len(axes):\n",
    "        band_data = stk.sel(band=band_name)\n",
    "        im = axes[i].imshow(band_data, cmap='viridis')\n",
    "        axes[i].set_title(f'{band_name}')\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], shrink=0.6)\n",
    "\n",
    "# Remove the last empty subplot\n",
    "if len(axes) > len(stk.band.values):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract training data\n",
    "\n",
    "This chunk takes the training data from QGIS and gives you a summary of the total number of objects in each class and returns them in a table.\n",
    "\n",
    "Try to get at least 4-5 different classes (e.g., bare earth, water, green vegetation, brown vegetation, different veg that can be identified, etc). More small polygons are generally thought to be superior to fewer large polygons. Make sure to not include any areas that don't belong to the class you are generating a training polygon for.\n",
    "\n",
    "The lower case n in the table is the number of pixels available in each training data class. If some of the samples are very low compared to others, you might want to consider going back to QGIS and adding a few more polygons of the underrepresented class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "# Function to extract pixel values from raster using vector polygons\n",
    "def extract_pixels(raster_stack, polygons_gdf):\n",
    "    extracted_data = []\n",
    "    \n",
    "    for idx, row in polygons_gdf.iterrows():\n",
    "        # Get the geometry\n",
    "        geom = [mapping(row.geometry)]\n",
    "        \n",
    "        # Create a mask for this polygon\n",
    "        mask = features.rasterize(\n",
    "            geom,\n",
    "            out_shape=raster_stack.shape[1:],\n",
    "            transform=raster_stack.rio.transform(),\n",
    "            fill=0,\n",
    "            default_value=1\n",
    "        )\n",
    "        \n",
    "        # Extract pixel values where mask is True\n",
    "        for band_name in raster_stack.band.values:\n",
    "            band_data = raster_stack.sel(band=band_name).values\n",
    "            masked_pixels = band_data[mask == 1]\n",
    "            \n",
    "            # Remove NaN values\n",
    "            masked_pixels = masked_pixels[~np.isnan(masked_pixels)]\n",
    "            \n",
    "            for pixel_val in masked_pixels:\n",
    "                extracted_data.append({\n",
    "                    'ID': idx,\n",
    "                    'class': row['class'],\n",
    "                    'band': band_name,\n",
    "                    'value': pixel_val\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "# Extract pixel values for training polygons\n",
    "print(\"Extracting training data... This may take a few minutes.\")\n",
    "extracted_df = extract_pixels(stk, trn)\n",
    "\n",
    "# Pivot the data so each row is a pixel and each column is a band\n",
    "smp = extracted_df.pivot_table(\n",
    "    index=['ID', 'class'], \n",
    "    columns='band', \n",
    "    values='value', \n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Remove rows with any NaN values\n",
    "smp = smp.dropna()\n",
    "\n",
    "print(f\"\\nExtracted {len(smp)} pixel samples\")\n",
    "print(\"\\nSamples per class:\")\n",
    "print(smp['class'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the training data\n",
    "\n",
    "In the next chunk, we will generate a few plots to assess how well defined the training data classes are. We talked about some of these in the lecture. Note that we normalized the digital numbers further up to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check per-class sample sizes\n",
    "class_counts = smp['class'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "plt.title('Number of Training Samples per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Pixel Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for long format (similar to pivot_longer in R)\n",
    "band_columns = ['R', 'G', 'B', 'VARI', 'ExG', 'NDI', 'GRVI']\n",
    "smp_long = pd.melt(smp, \n",
    "                   id_vars=['ID', 'class'], \n",
    "                   value_vars=band_columns,\n",
    "                   var_name='Band', \n",
    "                   value_name='Value')\n",
    "\n",
    "# Histograms per band & class\n",
    "fig, axes = plt.subplots(len(smp['class'].unique()), len(band_columns), \n",
    "                        figsize=(20, 4*len(smp['class'].unique())))\n",
    "\n",
    "if len(smp['class'].unique()) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, class_name in enumerate(sorted(smp['class'].unique())):\n",
    "    for j, band_name in enumerate(band_columns):\n",
    "        data_subset = smp_long[(smp_long['class'] == class_name) & \n",
    "                              (smp_long['Band'] == band_name)]['Value']\n",
    "        axes[i, j].hist(data_subset, bins=40, color='gray', alpha=0.7, edgecolor='black')\n",
    "        axes[i, j].set_title(f'{class_name} - {band_name}')\n",
    "        axes[i, j].set_xlabel('Digital Number (DN)')\n",
    "        axes[i, j].set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Histograms of Pixel Values by Class and Band', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overlaid density plots by band\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, band_name in enumerate(band_columns):\n",
    "    if i < len(axes):\n",
    "        for class_name in sorted(smp['class'].unique()):\n",
    "            data_subset = smp_long[(smp_long['class'] == class_name) & \n",
    "                                  (smp_long['Band'] == band_name)]['Value']\n",
    "            axes[i].hist(data_subset, bins=30, alpha=0.5, label=class_name, density=True)\n",
    "        axes[i].set_title(f'{band_name}')\n",
    "        axes[i].set_xlabel('Digital Number (DN)')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend()\n",
    "\n",
    "# Remove the last empty subplot\n",
    "if len(axes) > len(band_columns):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Spectral Distributions by Class', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots per band grouped by class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, band_name in enumerate(band_columns):\n",
    "    if i < len(axes):\n",
    "        band_data = smp_long[smp_long['Band'] == band_name]\n",
    "        sns.boxplot(data=band_data, x='class', y='Value', ax=axes[i])\n",
    "        axes[i].set_title(f'{band_name}')\n",
    "        axes[i].set_xlabel('Class')\n",
    "        axes[i].set_ylabel('Digital Number (DN)')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove the last empty subplot\n",
    "if len(axes) > len(band_columns):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Boxplots of DN Values by Band and Class', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest Model\n",
    "\n",
    "This chunk runs the actual RF model. For the model, we will use 80% of the available pixels in the training dataset and the remaining 20% in the verification dataset. This means that 80% of the pixels in each class can theoretically be used to improve the model and the model will then be tested on the remaining 20% of the pixels. We will use 301 random forest trees. Ideally you would probably want to choose more, but we will go with a more limited number due to the constraints of the lab computers.\n",
    "\n",
    "This chunk will both create the model that we will use for the classification and a couple things to evaluate how well the model actually works. The variable importance tells you about, well, how important each of our chosen bands is for the classification. The longer the bar, the more important that particular band or index is for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Prepare the data for sklearn\n",
    "X = smp[band_columns]  # Features (bands and indices)\n",
    "y = smp['class']       # Target (class labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Classes: {sorted(y.unique())}\")\n",
    "\n",
    "# Create stratified train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "# Train the Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=301,\n",
    "    random_state=7,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Extract and plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Variable'], feature_importance['Importance'])\n",
    "plt.title('Variable Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for var, imp in zip(feature_importance['Variable'], feature_importance['Importance']):\n",
    "    print(f\"{var}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = \"my_trained_rf_model.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back\n",
    "model_path = \"my_trained_rf_model.pkl\"\n",
    "with open(model_path, 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Assessment\n",
    "\n",
    "This chunk uses the test data (pixels) that we set aside earlier to use in a test run. We apply the model to the test dataset, where we know exactly what the correct classification should be. This allows us to estimate the general accuracy of the model down to the class level (i.e., do predictions work better for some classes than for others).\n",
    "\n",
    "This is where you tinker with the model. Well, not the actual model, but with the training data. If you see classes that aren't predicted well, go back to QGIS and check your training polygons. If classes are too similar, try to find training polygons that are more different. Or combine classes. Use your best knowledge of training data to refine the model outcome!\n",
    "\n",
    "The confusion matrix is really useful for checking how well the model predicts the individual classes. This is where the remaining 20% of the training data set come into play. On the Y axis, we have the actual class, and on the X axis we have the predicted class. Each row adds up to 100%. You want to see really high percentages for a given class and the prediction of the given class. If the training data are ambiguous for a class you will probably see that there are high percentages for each given class that are predicted incorrectly. If the majority of a class is misclassified, either consider dropping the class or creating, if possible, better training polygons for that class.\n",
    "\n",
    "The table gives you some additional classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "class_names = sorted(y_test.unique())\n",
    "\n",
    "# Normalize confusion matrix by row (actual class) to get percentages\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, \n",
    "            annot=True, \n",
    "            fmt='.1f', \n",
    "            cmap='Blues',\n",
    "            xticklabels=class_names, \n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Confusion Matrix (Normalized by Row - %)')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create a DataFrame with per-class metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-class Metrics:\")\n",
    "print(metrics_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the raster\n",
    "\n",
    "In this chunk, we apply the classification to the full image.\n",
    "\n",
    "THIS STEP IS GOING TO TAKE A WHILE! (20-30 minutes would be my guess; it took about 40 minutes on an 8-year old laptop, and 13 minutes on a very fast desktop computer) Note that the first code line in this chunk sets processing parameters. There is a possibility that the lab computers don't allow us to store data on the C drive. If that is the case, we may need to change it to a temp folder in our OneDrive. There is also a small chance it might crash your computer... Or just take way too long. If that is the case, I will run the classification for you on my office computer (after the lab) and send you the results (hopefully we can avoid this, though). If you need to do this, please keep all the necessary files in the same folder, compress the folder (right-click, compress as zip) and then email it to me. I will send you the completed classification back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classification for every pixel\n",
    "print(\"Starting full image classification...\")\n",
    "print(f\"Image dimensions: {stk.shape[1]} x {stk.shape[2]} pixels\")\n",
    "print(f\"Total pixels to classify: {stk.shape[1] * stk.shape[2]:,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Reshape the stack data for prediction\n",
    "# Convert from (bands, height, width) to (pixels, bands)\n",
    "stk_array = stk.values\n",
    "n_bands, n_rows, n_cols = stk_array.shape\n",
    "\n",
    "# Reshape to (n_pixels, n_bands)\n",
    "pixels_array = stk_array.reshape(n_bands, -1).T\n",
    "\n",
    "# Remove pixels with NaN values\n",
    "valid_mask = ~np.isnan(pixels_array).any(axis=1)\n",
    "valid_pixels = pixels_array[valid_mask]\n",
    "\n",
    "print(f\"Valid pixels for classification: {len(valid_pixels):,}\")\n",
    "\n",
    "# Predict on valid pixels\n",
    "if len(valid_pixels) > 0:\n",
    "    print(\"Applying Random Forest model...\")\n",
    "    predictions = rf_model.predict(valid_pixels)\n",
    "    \n",
    "    # Create full prediction array with NaN for invalid pixels\n",
    "    full_predictions = np.full(n_rows * n_cols, np.nan)\n",
    "    full_predictions[valid_mask] = predictions\n",
    "    \n",
    "    # Reshape back to image dimensions\n",
    "    class_array = full_predictions.reshape(n_rows, n_cols)\n",
    "    \n",
    "    # Create xarray DataArray for the classification result\n",
    "    class_raster = xr.DataArray(\n",
    "        class_array,\n",
    "        coords=[stk.y, stk.x],\n",
    "        dims=['y', 'x'],\n",
    "        attrs=stk.attrs\n",
    "    )\n",
    "    \n",
    "    # Save the classification result\n",
    "    class_raster.rio.write_crs(stk.rio.crs, inplace=True)\n",
    "    class_raster.rio.to_raster(\"classified_map_python.tif\")\n",
    "    \n",
    "    classification_time = time.time() - start_time\n",
    "    print(f\"Classification completed in {classification_time:.2f} seconds\")\n",
    "    print(\"Classification saved as 'classified_map_python.tif'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid pixels found for classification!\")\n",
    "\n",
    "# Plot the classification result\n",
    "if 'class_raster' in locals():\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a custom colormap for the classes\n",
    "    unique_classes = sorted([c for c in np.unique(class_raster.values) if not np.isnan(c)])\n",
    "    n_classes = len(unique_classes)\n",
    "    \n",
    "    # Plot the classification\n",
    "    im = plt.imshow(class_raster.values, cmap='Set3', vmin=0, vmax=n_classes-1)\n",
    "    \n",
    "    # Add colorbar with class labels\n",
    "    cbar = plt.colorbar(im, shrink=0.6)\n",
    "    cbar.set_label('Class')\n",
    "    \n",
    "    plt.title('Random Forest Classification of UAV RGB Imagery')\n",
    "    plt.xlabel('Easting')\n",
    "    plt.ylabel('Northing')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification summary\n",
    "    print(\"\\nClassification Summary:\")\n",
    "    unique_values, counts = np.unique(class_raster.values[~np.isnan(class_raster.values)], return_counts=True)\n",
    "    for val, count in zip(unique_values, counts):\n",
    "        print(f\"Class {int(val)}: {count:,} pixels ({count/np.sum(counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you are done with an image classification! Having the orthomosaic and classification in Python is good, but you can better evaluate the results if you copy both of the tifs into QGIS. Just note that you won't see the class names in QGIS, they will be labeled 1 through x, with x being your number of classes.\n",
    "\n",
    "## QUESTIONS\n",
    "\n",
    "**Q1 (5 pts):** Describe the process for supervised classification. Within this, briefly explain the random forest approach.\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "**Q2 (5 pts):** Discuss how well your classes are defined (what classes did you use and why?). Use the spectral plots for this answer.\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "**Q3 (5 pts):** Connected to Q2, how well does the model perform? Assess it using the tables and graphs that you generated (Variable Importance and Confusion Matrix).\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "**Q4 (4 pts):** Discuss your classification results in general outside of the training data evaluation. What were issues/difficulties during the classification process? How could you improve the classification?\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "**Q5 (5 pts):** In addition to answering the questions, include the resulting notebook file. That should ideally have all the plots and figures.\n",
    "\n",
    "*Answer:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}